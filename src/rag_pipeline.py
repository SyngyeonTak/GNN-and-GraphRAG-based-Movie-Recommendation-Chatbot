from dotenv import load_dotenv
import os
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain_community.graphs  import Neo4jGraph  # Use the new, recommended package

def clean_cypher_query(query: str) -> str:
    """
    Cleans the Cypher query generated by the LLM by removing Markdown code blocks.
    """
    if "```cypher" in query:
        # Extracts the query from a ```cypher code block
        query = query.split("```cypher", 1)[1]
    if "```" in query:
        # Removes the closing backticks
        query = query.split("```", 1)[0]
        
    # Removes leading/trailing whitespace and newlines
    return query.strip()

# ==================================================================
# 1. Environment Setup
# ==================================================================
def setup_environment():
    """
    Loads API keys and Neo4j credentials from .env, then returns LLM and Graph instances.
    """
    load_dotenv()
    
    uri = os.environ.get("NEO4J_URI")
    user = os.environ.get("NEO4J_USER")
    password = os.environ.get("NEO4J_PASSWORD")
    openai_api_key = os.environ.get("OPENAI_API_KEY")

    try:
        graph = Neo4jGraph(url=uri, username=user, password=password)
        print("‚úÖ Neo4j database connection successful!")
        print("Graph Schema:\n", graph.schema)
    except Exception as e:
        print(f"‚ùå Neo4j connection failed: {e}")
        print("Please check your NEO4J_URI, NEO4J_USER, and NEO4J_PASSWORD environment variables.")
        graph = None

    # The API key is automatically read from the OPENAI_API_KEY environment variable
    llm = ChatOpenAI(openai_api_key = openai_api_key, temperature=0, model_name="gpt-4o-mini")
    print("‚úÖ OpenAI LLM model initialized successfully!")
    
    return llm, graph

# ==================================================================
# 2. Hybrid Retriever Function Definitions
# ==================================================================

# 2-A. Query Router
def get_query_router_chain(llm):
    """
    Creates a chain to classify the user's query into one of three categories.
    """
    router_prompt_template = """
    You are a highly intelligent query classifier for a movie recommendation chatbot.
    Your task is to analyze the user's query and decide which tool to use.
    Pay close attention to whether the user is asking for a general recommendation or a recommendation with specific constraints (like actors, directors, genres).

    Here are the available tools:

    1. **fact_based_search**: Use this for queries that ask for specific facts OR recommendations with **specific constraints**.
       These queries usually contain named entities like movie titles, actors, directors, or specific properties like genres, release years.
       - Example: "Who directed the movie Inception?"
       - Example: "Show me action movies starring Tom Cruise."
       - Example: "Recommend a comedy movie from the 90s."

    2. **personalized_recommendation**: Use this for queries that are **open-ended**, ask for general suggestions, or are based on the user's personal taste without specific constraints.
       - Example: "What should I watch tonight?"
       - Example: "Recommend a movie for me."

    3. **chit_chat**: Use this for queries that are simple greetings, conversational fillers, or off-topic.
       - Example: "Hello"
       - Example: "What's the weather like today?"

    User Query: "{user_query}"

    Based on the query, which tool should be used? Respond with only the name of the tool.
    Tool:
    """
    router_prompt = PromptTemplate(template=router_prompt_template, input_variables=["user_query"])
    return LLMChain(llm=llm, prompt=router_prompt, verbose=False)

# 2-B. Fact-Based Search
def get_cypher_generation_chain(llm):
    """
    Creates a chain that generates a Cypher query based on the user's question and the graph schema.
    """
    cypher_generation_template = """
    Task: Generate a read-only Cypher query to retrieve information from a Neo4j database based on the user's question.
    Instructions:
    1. Use the provided schema to understand the graph structure.
    2. Respond with ONLY the Cypher query. Do not include any explanations.
    Schema: {schema}
    User Question: {question}
    Cypher Query:
    """
    cypher_prompt = PromptTemplate(template=cypher_generation_template, input_variables=["schema", "question"])
    return LLMChain(llm=llm, prompt=cypher_prompt, verbose=False)

def fact_based_search(user_query: str, graph, cypher_chain):
    """
    Converts a user's question into a Cypher query and executes it against the Neo4j database.
    """
    print("\n[Executing Fact-Based Search]")
    
    # 1. Generate the Cypher query using the LLM
    generated_text = cypher_chain.invoke({
        "schema": graph.schema, 
        "question": user_query
    })['text']

    # 2. Clean the generated text to get a pure Cypher query
    cleaned_query = clean_cypher_query(generated_text)
    
    print(f"üß† Generated Cypher Query (Cleaned): {cleaned_query}")

    # 3. Execute the cleaned query against the database
    try:
        result = graph.query(cleaned_query)
        print(f"‚úÖ Query executed successfully. Result: {result}")
        return result
    except Exception as e:
        print(f"‚ùå Query execution failed: {e}")
        return []

# 2-C. Personalized Recommendation
def personalized_recommendation(user_id, top_k=5):
    """
    (Placeholder) Generates personalized movie recommendations for a user based on a GNN model.
    """
    print("\n[Executing Personalized Recommendation]")
    print(f"ü§ñ Running GNN-based recommendation for User: {user_id}")
    # This is where the logic for your GNN model from Phase 1 will go.
    dummy_recommendations = [
        {"title": "Inception", "reason": "Because you liked 'The Dark Knight', which has a similar style."},
        {"title": "The Matrix", "reason": "As you seem to enjoy Sci-Fi movies."}
    ]
    print(f"‚úÖ Recommendation Result: {dummy_recommendations}")
    return dummy_recommendations

# 2-D. Chit-Chat
def chit_chat(user_query):
    """
    Handles simple, conversational, or off-topic queries.
    """
    print("\n[Handling Chit-Chat]")
    response = "I'm a movie recommendation chatbot. What kind of movie are you looking for? üòä"
    print(f"‚úÖ Response: {response}")
    return response

# 2-E. Hybrid Retriever Main Controller
def hybrid_retriever(user_query, llm, graph, router_chain, cypher_chain, user_id="new_user"):
    """
    The main controller for the hybrid retriever. It routes the query and calls the appropriate function.
    """
    print(f"\n--- User Query: '{user_query}' ---")
    
    # 1. Route the query to the appropriate tool
    route = router_chain.invoke({"user_query": user_query})['text'].strip()
    
    # 2. Execute the corresponding function based on the route
    if route == "fact_based_search":
        print("Route: Fact-Based Search")
        return fact_based_search(user_query, graph, cypher_chain)
    elif route == "personalized_recommendation":
        print("Route: Personalized Recommendation")
        return personalized_recommendation(user_id)
    elif route == "chit_chat":
        print("Route: Chit-Chat")
        return chit_chat(user_query)
    else:
        response = "I'm sorry, I couldn't understand your request. Could you please rephrase it?"
        print(f"Route: Unknown. Response: {response}")
        return response

# ==================================================================
# 3. Main Execution
# ==================================================================
def main():
    """
    Sets up the entire program and runs test scenarios.
    """
    print("üöÄ Starting Movie Recommendation Chatbot Hybrid Retriever Test...")
    print("="*60)
    
    # Setup environment
    llm, graph = setup_environment()
    if not graph:
        print("Exiting program due to setup failure.")
        return
        
    # Initialize LangChain chains
    query_router_chain = get_query_router_chain(llm)
    cypher_generation_chain = get_cypher_generation_chain(llm)
    
    print("\n" + "="*60)
    
    # --- Execute Test Scenarios ---
    
    # Scenario 1: Fact-Based Search
    hybrid_retriever(
        user_query="Recommend 3 movies starring Tom Hanks.",
        llm=llm,
        graph=graph,
        router_chain=query_router_chain,
        cypher_chain=cypher_generation_chain
    )
    
    # Scenario 2: Personalized Recommendation
    hybrid_retriever(
        user_query="What should I watch tonight?",
        llm=llm,
        graph=graph,
        router_chain=query_router_chain,
        cypher_chain=cypher_generation_chain
    )
    
    # Scenario 3: Chit-Chat
    hybrid_retriever(
        user_query="What's the weather like today?",
        llm=llm,
        graph=graph,
        router_chain=query_router_chain,
        cypher_chain=cypher_generation_chain
    )
    
    print("\n" + "="*60)
    print("‚úÖ All test scenarios have been executed.")


if __name__ == "__main__":
    main()